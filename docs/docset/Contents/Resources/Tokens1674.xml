<?xml version="1.0" encoding="UTF-8"?>
<Tokens version="1.0">
	<File path="Classes/AWSRekognitionCompareFacesResponse.html">
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/cl/AWSRekognitionCompareFacesResponse</TokenIdentifier>
			<Abstract type="html"></Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
            
			
			<NodeRef refid="1674"/>
		</Token>
		
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/setFaceMatches:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of faces in the target image that match the source image face. Each &lt;code&gt;CompareFacesMatch&lt;/code&gt; object provides the bounding box, the confidence level that the bounding box contains a face, and the similarity score for the face in the bounding box and the face in the source image.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionCompareFacesMatch*&gt; *faceMatches</Declaration>
			
			
			<Anchor>//api/name/faceMatches</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/faceMatches</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of faces in the target image that match the source image face. Each &lt;code&gt;CompareFacesMatch&lt;/code&gt; object provides the bounding box, the confidence level that the bounding box contains a face, and the similarity score for the face in the bounding box and the face in the source image.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionCompareFacesMatch*&gt; *faceMatches</Declaration>
			
			
			<Anchor>//api/name/faceMatches</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionCompareFacesResponse/faceMatches</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of faces in the target image that match the source image face. Each &lt;code&gt;CompareFacesMatch&lt;/code&gt; object provides the bounding box, the confidence level that the bounding box contains a face, and the similarity score for the face in the bounding box and the face in the source image.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionCompareFacesMatch*&gt; *faceMatches</Declaration>
			
			
			<Anchor>//api/name/faceMatches</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/setSourceImageFace:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;The face in the source image that was used for comparison.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) AWSRekognitionComparedSourceImageFace *sourceImageFace</Declaration>
			
			
			<Anchor>//api/name/sourceImageFace</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/sourceImageFace</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;The face in the source image that was used for comparison.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) AWSRekognitionComparedSourceImageFace *sourceImageFace</Declaration>
			
			
			<Anchor>//api/name/sourceImageFace</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionCompareFacesResponse/sourceImageFace</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;The face in the source image that was used for comparison.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) AWSRekognitionComparedSourceImageFace *sourceImageFace</Declaration>
			
			
			<Anchor>//api/name/sourceImageFace</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/setSourceImageOrientationCorrection:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt; The orientation of the source image (counterclockwise direction). If your application displays the source image, you can use this value to correct image orientation. The bounding box coordinates returned in &lt;code&gt;SourceImageFace&lt;/code&gt; represent the location of the face before the image orientation is corrected. &lt;/p&gt;&lt;note&gt;&lt;p&gt;If the source image is in .jpeg format, it might contain exchangeable image (Exif) metadata that includes the image&apos;s orientation. If the Exif metadata for the source image populates the orientation field, the value of &lt;code&gt;OrientationCorrection&lt;/code&gt; is null and the &lt;code&gt;SourceImageFace&lt;/code&gt; bounding box coordinates represent the location of the face after Exif metadata is used to correct the orientation. Images in .png format don&apos;t contain Exif metadata.&lt;/p&gt;&lt;/note&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionOrientationCorrection sourceImageOrientationCorrection</Declaration>
			
			
			<Anchor>//api/name/sourceImageOrientationCorrection</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/sourceImageOrientationCorrection</TokenIdentifier>
			<Abstract type="html">&lt;p&gt; The orientation of the source image (counterclockwise direction). If your application displays the source image, you can use this value to correct image orientation. The bounding box coordinates returned in &lt;code&gt;SourceImageFace&lt;/code&gt; represent the location of the face before the image orientation is corrected. &lt;/p&gt;&lt;note&gt;&lt;p&gt;If the source image is in .jpeg format, it might contain exchangeable image (Exif) metadata that includes the image&apos;s orientation. If the Exif metadata for the source image populates the orientation field, the value of &lt;code&gt;OrientationCorrection&lt;/code&gt; is null and the &lt;code&gt;SourceImageFace&lt;/code&gt; bounding box coordinates represent the location of the face after Exif metadata is used to correct the orientation. Images in .png format don&apos;t contain Exif metadata.&lt;/p&gt;&lt;/note&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionOrientationCorrection sourceImageOrientationCorrection</Declaration>
			
			
			<Anchor>//api/name/sourceImageOrientationCorrection</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionCompareFacesResponse/sourceImageOrientationCorrection</TokenIdentifier>
			<Abstract type="html">&lt;p&gt; The orientation of the source image (counterclockwise direction). If your application displays the source image, you can use this value to correct image orientation. The bounding box coordinates returned in &lt;code&gt;SourceImageFace&lt;/code&gt; represent the location of the face before the image orientation is corrected. &lt;/p&gt;&lt;note&gt;&lt;p&gt;If the source image is in .jpeg format, it might contain exchangeable image (Exif) metadata that includes the image&apos;s orientation. If the Exif metadata for the source image populates the orientation field, the value of &lt;code&gt;OrientationCorrection&lt;/code&gt; is null and the &lt;code&gt;SourceImageFace&lt;/code&gt; bounding box coordinates represent the location of the face after Exif metadata is used to correct the orientation. Images in .png format don&apos;t contain Exif metadata.&lt;/p&gt;&lt;/note&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionOrientationCorrection sourceImageOrientationCorrection</Declaration>
			
			
			<Anchor>//api/name/sourceImageOrientationCorrection</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/setTargetImageOrientationCorrection:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt; The orientation of the target image (in counterclockwise direction). If your application displays the target image, you can use this value to correct the orientation of the image. The bounding box coordinates returned in &lt;code&gt;FaceMatches&lt;/code&gt; and &lt;code&gt;UnmatchedFaces&lt;/code&gt; represent face locations before the image orientation is corrected. &lt;/p&gt;&lt;note&gt;&lt;p&gt;If the target image is in .jpg format, it might contain Exif metadata that includes the orientation of the image. If the Exif metadata for the target image populates the orientation field, the value of &lt;code&gt;OrientationCorrection&lt;/code&gt; is null and the bounding box coordinates in &lt;code&gt;FaceMatches&lt;/code&gt; and &lt;code&gt;UnmatchedFaces&lt;/code&gt; represent the location of the face after Exif metadata is used to correct the orientation. Images in .png format don&apos;t contain Exif metadata.&lt;/p&gt;&lt;/note&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionOrientationCorrection targetImageOrientationCorrection</Declaration>
			
			
			<Anchor>//api/name/targetImageOrientationCorrection</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/targetImageOrientationCorrection</TokenIdentifier>
			<Abstract type="html">&lt;p&gt; The orientation of the target image (in counterclockwise direction). If your application displays the target image, you can use this value to correct the orientation of the image. The bounding box coordinates returned in &lt;code&gt;FaceMatches&lt;/code&gt; and &lt;code&gt;UnmatchedFaces&lt;/code&gt; represent face locations before the image orientation is corrected. &lt;/p&gt;&lt;note&gt;&lt;p&gt;If the target image is in .jpg format, it might contain Exif metadata that includes the orientation of the image. If the Exif metadata for the target image populates the orientation field, the value of &lt;code&gt;OrientationCorrection&lt;/code&gt; is null and the bounding box coordinates in &lt;code&gt;FaceMatches&lt;/code&gt; and &lt;code&gt;UnmatchedFaces&lt;/code&gt; represent the location of the face after Exif metadata is used to correct the orientation. Images in .png format don&apos;t contain Exif metadata.&lt;/p&gt;&lt;/note&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionOrientationCorrection targetImageOrientationCorrection</Declaration>
			
			
			<Anchor>//api/name/targetImageOrientationCorrection</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionCompareFacesResponse/targetImageOrientationCorrection</TokenIdentifier>
			<Abstract type="html">&lt;p&gt; The orientation of the target image (in counterclockwise direction). If your application displays the target image, you can use this value to correct the orientation of the image. The bounding box coordinates returned in &lt;code&gt;FaceMatches&lt;/code&gt; and &lt;code&gt;UnmatchedFaces&lt;/code&gt; represent face locations before the image orientation is corrected. &lt;/p&gt;&lt;note&gt;&lt;p&gt;If the target image is in .jpg format, it might contain Exif metadata that includes the orientation of the image. If the Exif metadata for the target image populates the orientation field, the value of &lt;code&gt;OrientationCorrection&lt;/code&gt; is null and the bounding box coordinates in &lt;code&gt;FaceMatches&lt;/code&gt; and &lt;code&gt;UnmatchedFaces&lt;/code&gt; represent the location of the face after Exif metadata is used to correct the orientation. Images in .png format don&apos;t contain Exif metadata.&lt;/p&gt;&lt;/note&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionOrientationCorrection targetImageOrientationCorrection</Declaration>
			
			
			<Anchor>//api/name/targetImageOrientationCorrection</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/setUnmatchedFaces:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of faces in the target image that did not match the source image face.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionComparedFace*&gt; *unmatchedFaces</Declaration>
			
			
			<Anchor>//api/name/unmatchedFaces</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionCompareFacesResponse/unmatchedFaces</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of faces in the target image that did not match the source image face.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionComparedFace*&gt; *unmatchedFaces</Declaration>
			
			
			<Anchor>//api/name/unmatchedFaces</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionCompareFacesResponse/unmatchedFaces</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of faces in the target image that did not match the source image face.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionComparedFace*&gt; *unmatchedFaces</Declaration>
			
			
			<Anchor>//api/name/unmatchedFaces</Anchor>
            <NodeRef refid="1674"/>
		</Token>
		
        
        
	</File>
</Tokens>